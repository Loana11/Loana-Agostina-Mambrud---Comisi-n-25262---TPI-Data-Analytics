# -*- coding: utf-8 -*-
"""PreEntrega.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UuBMAKi43196mgVgo9ryFIiu0IPx_74k

# üìò Pre-entrega

**Nombre del alumno:** Loana Agostina Mambrud

## üß© Etapa 1: Recopilaci√≥n y Preparaci√≥n de Datos
**Objetivo:** Demostrar habilidades en Python, familiaridad con el entorno de trabajo y conocimientos b√°sicos sobre manipulaci√≥n de datos.

### 1Ô∏è‚É£ Carga de datos
"""

# TODO: Cargar los datasets como DataFrames usando pandas.
# Sugerencia: utilizar pd.read_csv() para cargar los archivos CSV.
# Record√° verificar la ruta correcta y revisar las primeras filas con .head().


import pandas as pd
import numpy as np
import os
from google.colab import drive


drive.mount('/content/drive')
os.listdir("/content/drive/MyDrive/datasets")

ruta_ventas = "/content/drive/MyDrive/datasets/ventas.csv"
ruta_clientes = "/content/drive/MyDrive/datasets/clientes.csv"
ruta_marketing = "/content/drive/MyDrive/datasets/marketing.csv"

ventas = pd.read_csv(ruta_ventas)
clientes = pd.read_csv(ruta_clientes)
marketing = pd.read_csv(ruta_marketing)

print("ventas.shape ->", ventas.shape)
print("clientes.shape ->", clientes.shape)
print("marketing.shape ->", marketing.shape)

display(ventas.head(3))
display(clientes.head(3))
display(marketing.head(3))

"""### 2Ô∏è‚É£ An√°lisis exploratorio inicial"""

# TODO: Realizar un an√°lisis exploratorio inicial de los DataFrames.
# Sugerencia: usar m√©todos como .info(), .describe(), .shape y .columns. dtypes.
# Esto te ayudar√° a entender la estructura y el contenido de los datos.
# Pod√©s incluir comentarios sobre observaciones iniciales.


def eda(df, nombre):
    print(f"=== {nombre} ===")
    print("shape:", df.shape)
    print("columnas:", list(df.columns))
    print("dtypes:")
    print(df.dtypes)
    print("\nNulos por columna:")
    print(df.isna().sum())
    print("\nPrimeras filas:")
    display(df.head(5))
    print("\nDescribe (num√©rico):")
    display(df.describe(include='number'))
    print("-"*100)

eda(ventas, "VENTAS (inicial)")
eda(clientes, "CLIENTES (inicial)")
eda(marketing, "MARKETING (inicial)")

"""### 3Ô∏è‚É£ Calidad de los datos"""

# TODO: Identificar valores nulos y duplicados en los conjuntos de datos.
# Sugerencia: usar .isna().sum() y .duplicated().sum().
# Documentar las observaciones con print() o comentarios.

def calidad(df, nombre, clave=None):
    """
    Analiza la calidad del DataFrame:
      - Muestra cantidad de nulos por columna.
      - Cuenta filas duplicadas completas.
      - Si se indica una clave, muestra los valores duplicados m√°s frecuentes.
    Par√°metros:
      df: DataFrame de pandas que se analizar√°.
      nombre: texto descriptivo del DataFrame (ejemplo: 'VENTAS').
      clave: (opcional) nombre de la columna para buscar duplicados espec√≠ficos.
    """

    # -------------------------------------------------
    # Mostrar t√≠tulo descriptivo con el nombre del DF
    # -------------------------------------------------
    print(f"### {nombre}")

    # -------------------------------------------------
    # Mostrar cantidad de valores nulos por columna
    # -------------------------------------------------
    # df.isna() devuelve un DataFrame booleano con True donde hay NaN.
    # .sum() cuenta los True (o sea, los nulos) por columna.
    # .to_frame("nulos") convierte el resultado en un DataFrame con una columna llamada 'nulos'.
    display(df.isna().sum().to_frame("nulos"))

    # -------------------------------------------------
    # Contar filas duplicadas completas
    # -------------------------------------------------
    # df.duplicated(keep=False) marca como True todas las filas que tienen otra igual.
    # keep=False significa que marca todas las copias, no solo una.
    # .sum() cuenta cu√°ntas filas est√°n repetidas.
    dup_rows = df.duplicated(keep=False).sum()
    print("Filas duplicadas (exactas):", dup_rows)

    # -------------------------------------------------
    # Si se especific√≥ una columna clave v√°lida, analizar duplicados por esa columna
    # -------------------------------------------------
    # if clave analiza que clave no sea None
    # and (y)
    if clave and clave in df.columns:
    # clave in df.columns-- >que clave sea una columna existente dentro de las columnas del dataframe
    # si no le paso ninguna columna no va a querer encontrar duplicados por columna
    # y si me equivoco y le paso una columna que no existe en el dataframe, tampoco ingresara al if.
        # Contar cu√°ntas filas tienen valores repetidos en esa columna
        dup_key = df[clave].duplicated(keep=False).sum()
        print(f"Duplicados por clave '{clave}':", dup_key)

        # Si existen duplicados, mostrar cu√°les son los valores m√°s repetidos
        if dup_key > 0:
            # Filtrar filas donde esa clave est√© duplicada
            # df[clave].duplicated(keep=False) devuelve True donde el valor se repite
            duplicados_ordenados = (
                df[df[clave].duplicated(keep=False)][clave]
                .value_counts()                # Cuenta cu√°ntas veces aparece cada valor
                .sort_values(ascending=False)   # Ordena de mayor a menor (m√°s duplicados arriba)
            )

            print("\nüîÅ Top valores duplicados m√°s frecuentes:")
            # Mostrar solo los primeros 10 (los m√°s repetidos)
            display(duplicados_ordenados.head(10))
        else:
            print(f"No se encontraron duplicados en la clave '{clave}'.")
    else:
        # Si la clave no fue pasada o no existe en el DataFrame
        if clave:
            print(f"La clave '{clave}' no existe en el DataFrame.")
        else:
            print("No se indic√≥ una clave para analizar duplicados por columna.")

calidad(ventas, "VENTAS", clave="id_venta")
calidad(clientes, "CLIENTES", clave="id_cliente")
calidad(marketing, "MARKETING", clave="id_campanha")

"""## üßπ Etapa 2: Preprocesamiento y Limpieza de Datos
**Objetivo:** Demostrar conocimiento de las t√©cnicas de limpieza y transformaci√≥n de datos.

### 4Ô∏è‚É£ Limpieza de datos
"""

# TODO: Limpiar el conjunto de datos eliminando duplicados y caracteres no deseados.
# Sugerencia: aplicar .drop_duplicates(), .str.strip(), .str.replace() seg√∫n el caso.
# Documentar el proceso y los resultados.


# ============================================
# üßπ LIMPIEZA Y NORMALIZACI√ìN DE LOS DATASETS
# ============================================
# Se limpian y normalizan los DataFrames:
#   ventas, clientes, marketing
# ============================================

# -------------------------------------------------
# 1Ô∏è‚É£ Crear copias independientes para no modificar los originales
# -------------------------------------------------
ventas_clean = ventas.copy()
clientes_clean = clientes.copy()
marketing_clean = marketing.copy()

# -------------------------------------------------
# 2Ô∏è‚É£ Eliminar filas completamente duplicadas
# -------------------------------------------------
ventas_clean = ventas_clean.drop_duplicates()
clientes_clean = clientes_clean.drop_duplicates()
marketing_clean = marketing_clean.drop_duplicates()

calidad(ventas_clean, "VENTAS CLEAN", clave="id_venta")
calidad(clientes_clean, "CLIENTES CLEAN", clave="id_cliente")
calidad(marketing_clean, "MARKETING CLEAN", clave="id_campanha")

# -------------------------------------------------
# 3Ô∏è‚É£ Funci√≥n para limpiar texto en columnas tipo string
# -------------------------------------------------
def normalizar_texto(df):
    for col in df.select_dtypes(include="object").columns:
        # Se agrupan las operaciones entre par√©ntesis () para escribirlas en varias l√≠neas
        # Python eval√∫a todo el bloque como una √∫nica expresi√≥n.
        df[col] = (
            df[col]
            .astype(str)                              # Convierte cualquier tipo a string
            # .astype(str)  ‚Üí convierte todo a texto; no tiene par√°metros adicionales.
            .str.strip()                               # Elimina espacios al inicio y final
            # .str.strip() no necesita argumentos; borra espacios en blanco por defecto.
            .str.replace(r"[\u200b\t\r\n]", "", regex=True)
            # .str.replace(patron, reemplazo, regex=True)
            #   patron: expresi√≥n regular que busca caracteres invisibles (\u200b, tabulaciones, saltos)
            #   reemplazo: ""  ‚Üí los elimina
            #   regex=True indica que 'patron' es una expresi√≥n regular.
            .str.replace(" +", " ", regex=True)
            # reemplaza "uno o m√°s espacios consecutivos" por un solo espacio
            .str.title()                               # Convierte a T√≠tulo: "juan p√©rez" ‚Üí "Juan P√©rez"
        )
        #df_transformado=df[col].astype(str)
        #df_transformado=df_transformado.str.strip()
        #df_transformado=df_transformado.str.replace(r"[\u200b\t\r\n]", "", regex=True)
        #df_transformado=df_transformado.str.replace(" +", " ", regex=True)
        #df_transformado=df_transformado.str.title()
        #df[col]=df_transformado

        #df[col] = df[col].astype(str).str.strip().str.replace(r"[\u200b\t\r\n]", "", regex=True).str.replace(" +", " ", regex=True).str.title()
    return df

#Normalizo las fechas teniendo en cuenta su respectivo nombre de columna dentro del dataframe

ventas_clean["fecha_venta"] = pd.to_datetime(ventas_clean["fecha_venta"], errors="coerce", dayfirst=True)
marketing_clean["fecha_inicio"] = pd.to_datetime(marketing_clean["fecha_inicio"], errors="coerce", dayfirst=True)
marketing_clean["fecha_fin"] = pd.to_datetime(marketing_clean["fecha_fin"], errors="coerce", dayfirst=True)

print(ventas_clean.dtypes)
print(clientes_clean.dtypes)
print(marketing_clean.dtypes)

# -------------------------------------------------
#  Aplicar la normalizaci√≥n de texto
# -------------------------------------------------
ventas_clean = normalizar_texto(ventas_clean)
clientes_clean = normalizar_texto(clientes_clean)
marketing_clean = normalizar_texto(marketing_clean)

#mostramos los df luego de normalizar los textos para revisar que queden bien
print(ventas_clean.head(10))
print(clientes_clean.head(10))
print(marketing_clean.head(10))

# -------------------------------------------------
# 6Ô∏è‚É£ Normalizar valores num√©ricos
# -------------------------------------------------
# üè∑Ô∏è Campo "precio"
if "precio" in ventas_clean.columns:
    # Se usa nuevamente agrupaci√≥n con () para encadenar m√©todos y mantener legibilidad
    ventas_clean["precio"] = (
        ventas_clean["precio"]
        .astype(str)                        # Convierte todo a texto
        .str.replace("$", "", regex=False)  # Elimina el s√≠mbolo $
        #   "$" ‚Üí texto literal a reemplazar
        #   ""  ‚Üí nuevo valor (vac√≠o)
        #   regex=False ‚Üí interpreta "$" literalmente, no como expresi√≥n regular
        .str.replace(",", "", regex=False)  # Elimina comas de miles 1,000  1000
        .str.strip()                        # Quita espacios sobrantes
    )
    ventas_clean["precio"] = pd.to_numeric(ventas_clean["precio"], errors="coerce")
    # pd.to_numeric convierte texto a n√∫mero (float o int)
    # Par√°metros:
    #   errors="coerce" ‚Üí reemplaza valores no convertibles con NaN

print(ventas_clean.columns)

# üßÆ Campo "cantidad"
if "cantidad" in ventas_clean.columns:
    ventas_clean["cantidad"] = pd.to_numeric(
        ventas_clean["cantidad"], errors="coerce"
    ).astype("Int64")
    # .astype("Int64") usa el tipo entero de pandas que permite valores nulos (NaN)

print(ventas_clean.dtypes)

print(ventas_clean.select_dtypes(include="object").columns)

# -------------------------------------------------
# 7Ô∏è‚É£ Guardar los DataFrames limpios como CSV
# -------------------------------------------------
#print(ventas_clean.head())
#print(clientes_clean.head())
#print(marketing_clean.head())
ventas_clean.info()
ventas_clean.to_csv("/content/drive/MyDrive/datasets/ventas_clean.csv", index=False)
clientes_clean.to_csv("/content/drive/MyDrive/datasets/clientes_clean.csv", index=False)
marketing_clean.to_csv("/content/drive/MyDrive/datasets/marketing_clean.csv", index=False)

print("‚úÖ Archivos guardados: ventas_clean.csv, clientes_clean.csv, marketing_clean.csv")

"""### 5Ô∏è‚É£ Transformaci√≥n de datos"""

# TODO: Aplicar filtros y transformaciones para crear una tabla de ventas
# que muestre solo los productos con alto rendimiento. calcular el percentil 80
# y filtrar los productos que superen ese umbral en ventas.
#quantile(0.8)
# Sugerencia: usar .query() o condiciones con operadores l√≥gicos.

# ============================================
# 7) TRANSFORMACI√ìN: productos de alto rendimiento
# ============================================
# Objetivo:
# - Detectar los productos con mejor desempe√±o econ√≥mico (top 20% por ingreso total).
# - Aplicar transformaci√≥n: calcular ingreso, agregar por producto y filtrar.
# ============================================

def encontrar_columna(df, candidatos):
    """
    Busca la primera columna cuyo nombre contenga alguno de los patrones dados.
    - df: DataFrame de pandas.
    - candidatos: lista de patrones (min√∫sculas).
    """
    # Recorremos todas las columnas del DataFrame
    for c in df.columns:

        # Convertimos el nombre de la columna a min√∫sculas
        # Esto se hace para comparar sin importar si est√° escrito con may√∫sculas o min√∫sculas
        nombre = c.lower()

        # Verificamos si alguna palabra (patr√≥n) de la lista 'candidatos'
        # est√° contenida dentro del nombre de la columna
        # 'any()' devuelve True apenas encuentra una coincidencia
        if any(p in nombre for p in candidatos):
            # Si encontramos una coincidencia, devolvemos el nombre original de la columna
            return c

    # Si termina el bucle y no se encontr√≥ ninguna coincidencia, devolvemos None
    return None

# 1Ô∏è‚É£ Detectar la columna de producto
prod_col = encontrar_columna(ventas_clean, ["producto", "id_producto", "sku", "articulo", "art√≠culo"])
if prod_col is None:
    raise ValueError("No se encontr√≥ columna de producto. Renombr√° una columna a 'producto' o similar.")
print(prod_col)

# 2Ô∏è‚É£ Calcular ingreso por registro = precio * cantidad
#() es para escribir en varias filas
ventas_perf = (
    ventas_clean
    .assign(
        ingreso = ventas_clean["precio"] * ventas_clean["cantidad"]
        # assign(**nuevas_col): crea nuevas columnas y devuelve una copia del DF.
        # Alternativa: ventas_clean["ingreso"] = ventas_clean["precio"] * ventas_clean["cantidad"]
    )
)
#esta linea comentada es igual que la linea multiple de arriba
#ventas_perf = ventas_clean.assign(ingreso = ventas_clean["precio"] * ventas_clean["cantidad"])
#esta otra linea agrega a ventas_clean una columna nueva ingreso y le asigna precio*cantidad
#ventas_clean["ingreso"] = ventas_clean["precio"] * ventas_clean["cantidad"]

# 3Ô∏è‚É£ Agregar m√©tricas por producto
resumen_prod = (
    ventas_perf
    # 1) Agrupamos el DataFrame por una o varias columnas clave
    .groupby(
        by=prod_col,    # Columna (str) o lista de columnas (list[str]) que define los grupos.
        dropna=False,   # False ‚Üí NO descarta filas donde la clave de grupo tenga NaN; crea un grupo para NaN.
        as_index=False, # False ‚Üí las columnas de agrupaci√≥n quedan como columnas normales (no pasan al √≠ndice).
        observed=False  # Solo aplica si 'prod_col' es Categorical:
                        #   False ‚Üí incluye categor√≠as NO observadas (posibles pero sin filas);
                        #   True  ‚Üí solo categor√≠as que aparecen en los datos (m√°s r√°pido y ‚Äúcompacto‚Äù).
    )
    # 2) Agregamos (resumimos) columnas num√©ricas por cada grupo
    .agg(
        ingreso_total=('ingreso', 'sum'),   # Suma de 'ingreso' por grupo (skipna=True por defecto).
        unidades=('cantidad', 'sum'),       # Suma de 'cantidad' por grupo.
        precio_promedio=('precio', 'mean'), # Promedio simple de 'precio' por grupo (ignora NaN).
        registros=('ingreso', 'size')       # N√∫mero de filas en el grupo (cuenta TODO, incluso NaN).
    )
)
#se puede escribir asi:
resumen_prod = ventas_perf.groupby(by=prod_col).agg(ingreso_total=('ingreso', 'sum'), unidades=('cantidad', 'sum'), precio_promedio=('precio', 'mean'), registros=('ingreso', 'size'))

# 4Ô∏è‚É£ Calcular percentil 80 de ingreso_total
# --------------------------------------------------------
# La funci√≥n quantile() nos permite obtener el valor de un percentil.
# En este caso, queremos saber el ingreso que separa al 80% de los productos
# con menores ingresos del 20% con mayores ingresos.

p80_ingreso = resumen_prod["ingreso_total"].quantile(
    q=0.80,                # q indica el percentil deseado (0.80 = 80% de los datos por debajo)
    interpolation="linear" # si el percentil no coincide exactamente con un valor real del dataset,
                           # 'linear' interpola entre los dos valores vecinos.
                           # Ejemplo: si el 80% cae entre 4000 y 5000,
                           # calcula un valor proporcional, por ejemplo 4200.
                           # Otros m√©todos posibles:
                           #  - 'lower': toma el menor de los dos valores (4000)
                           #  - 'higher': toma el mayor (5000)
                           #  - 'nearest': el m√°s cercano al percentil
                           #  - 'midpoint': el punto medio exacto (4500)
)

# En resumen:
# - quantile calcula el valor l√≠mite de un percentil.
# - q define qu√© percentil queremos.
# - interpolation define c√≥mo se calcula cuando el valor no est√° exactamente en los datos.
# El resultado (p80_ingreso) es el ingreso total que marca el l√≠mite superior del 80% de los productos.


# 5Ô∏è‚É£ Filtrar los productos "de alto rendimiento" y ordenarlos
# -------------------------------------------------------------------
# Contexto: `resumen_prod` es un DataFrame con m√©tricas por producto,
# y `p80_ingreso` es el percentil 80 de la columna "ingreso_total".
# Objetivo: quedarnos con los productos cuyo ingreso_total est√° en el 20% superior
# (ingreso_total >= p80_ingreso) y luego ordenarlos de mayor a menor por ingreso y unidades.
# en una sola fila
#ventas_top = resumen_prod.query("ingreso_total >= @p80_ingreso",engine="python").sort_values(by=["ingreso_total", "unidades"], ascending=[False, False], na_position="last", ignore_index=True
ventas_top = (
    resumen_prod
    # ---------------------------------------------------------------
    # .query(expr, *, inplace=False, engine='python'|'numexpr')
    #   - Aplica un filtro usando una expresi√≥n estilo SQL-simple.
    #   - `expr` es un string que se eval√∫a sobre los nombres de las columnas.
    #   - Para usar variables de Python (no columnas), se antepone '@' (ej.: @p80_ingreso).
    #   - NaN en comparaciones (>, >=, ==, etc.) se comportan como False ‚Üí esas filas no pasan el filtro.
    #   - engine='python': interpreta la expresi√≥n con Python puro (compatible siempre).
    #   - engine='numexpr': si est√° instalado, acelera operaciones num√©ricas vectorizadas.
    #   - inplace: False (por defecto) devuelve un DF nuevo; True modifica el DF original (menos recomendado en cadenas).
    .query(
        "ingreso_total >= @p80_ingreso",  # expr: filtra filas donde ingreso_total es al menos el umbral del P80
        engine="python"                   # motor de evaluaci√≥n (usar 'numexpr' si lo ten√©s y quer√©s performance)
        # Notas de sintaxis de `expr`:
        #   ‚Ä¢ Operadores l√≥gicos: and / or / not   (tambi√©n valen &, |, ~ con par√©ntesis).
        #   ‚Ä¢ Strings deben ir entre comillas: canal == 'Online'
        #   ‚Ä¢ Columnas con espacios o caracteres raros: usar `backticks`, ej.: `nombre producto` == 'X'
        #   ‚Ä¢ Ejemplos:
        #       "ingreso_total >= @p80_ingreso and unidades >= 10"
        #       "`nombre producto`.str.contains('Promo')"
        #       "precio_promedio.between(1000, 3000, inclusive='both')"
    )
    # ---------------------------------------------------------------
    # .sort_values(by, axis=0, ascending=True|[...], inplace=False,
    #              kind='quicksort'|'mergesort'|'heapsort'|'stable',
    #              na_position='last'|'first', ignore_index=False, key=None)
    #   - Ordena por una o varias columnas.
    #   - `by`: str o lista de str con las columnas a ordenar.
    #   - `ascending`: bool o lista de bool (una por cada columna en `by`).
    #   - `na_position`: d√≥nde ubicar NaN ('last' o 'first').
    #   - `ignore_index`: si True, reasigna el √≠ndice 0..n-1 en el resultado.
    #   - `kind`: algoritmo de ordenamiento (mergesort es estable si necesit√°s preservar empates).
    #   - `key`: funci√≥n que transforma los valores antes de ordenar (p. ej., key=lambda s: s.str.normalize(...)).
    .sort_values(
        by=["ingreso_total", "unidades"],  # primero ordena por ingreso_total, luego desempata por unidades
        ascending=[False, False],          # ambos en orden descendente (mayor ‚Üí menor)
        na_position="last",                # coloca NaN al final (√∫til si alguna m√©trica qued√≥ en NaN)
        ignore_index=True                  # reindexa el resultado secuencialmente (0..n-1)
        # Variantes √∫tiles:
        #   ‚Ä¢ ascending=True                 # orden ascendente
        #   ‚Ä¢ ascending=[False, True]        # primero desc, luego asc para el segundo criterio
        #   ‚Ä¢ kind='mergesort'               # orden estable (respeta el orden de aparici√≥n en empates)
        #   ‚Ä¢ key=lambda s: s.str.lower()    # ordenar texto sin distinci√≥n de may√∫sculas/min√∫sculas
    )
)

# Resultado:
# `ventas_top` contiene solo los productos cuyo ingreso_total >= p80_ingreso,
# ordenados de mayor a menor por ingreso_total y, ante empates, por unidades.


# 6Ô∏è‚É£ Mostrar resultados
print(f"Columna de producto detectada: {prod_col}")
print(f"Umbral (percentil 80) de ingreso_total: {float(p80_ingreso):,.2f}")
print("‚úÖ Productos de ALTO RENDIMIENTO (top 20% por ingreso):")
display(ventas_top.head(20))

#ESTO ES SOLO UN EJEMPLO PARA ENTENDER PERCENTIL Y NO TIENE NADA QUE VER CON LA ENTREGA!!!

# Lista de ingresos ordenados
ingresos = pd.Series([100, 200, 300, 400, 500])

# Calculamos el percentil 80 (P80)
# ---------------------------------------------------------
# Interpolar significa estimar un valor intermedio entre dos puntos conocidos.
# El percentil 80 no siempre coincide con un valor exacto del dataset,
# sino que puede "caer entre" dos posiciones.
#
# F√≥rmula de posici√≥n usada por pandas:
#   posici√≥n = (n - 1) * q
# En este caso: (5 - 1) * 0.8 = 3.2 ‚Üí est√° entre los √≠ndices 3 (valor 400) y 4 (valor 500)
#
# Interpolaci√≥n lineal:
#   400 + 0.2 * (500 - 400) = 420

p80_linear   = ingresos.quantile(0.8, interpolation='linear')
p80_lower    = ingresos.quantile(0.8, interpolation='lower')
p80_higher   = ingresos.quantile(0.8, interpolation='higher')
p80_nearest  = ingresos.quantile(0.8, interpolation='nearest')
p80_midpoint = ingresos.quantile(0.8, interpolation='midpoint')

print("Lista de ingresos:", list(ingresos))
print("\nPercentil 80 con diferentes m√©todos de interpolaci√≥n:\n")
print(f"Linear   ‚Üí {p80_linear}   (interpola ‚Üí 420)")
print(f"Lower    ‚Üí {p80_lower}    (toma el menor ‚Üí 400)")
print(f"Higher   ‚Üí {p80_higher}   (toma el mayor ‚Üí 500)")
print(f"Nearest  ‚Üí {p80_nearest}  (toma el m√°s cercano ‚Üí 400)")
print(f"Midpoint ‚Üí {p80_midpoint} (punto medio ‚Üí 450)")

# En resumen:
# - 'linear' estima el valor intermedio (420).
# - 'lower', 'higher', 'nearest', 'midpoint' eligen seg√∫n criterio fijo.
# - Interpolar = estimar un valor entre los existentes seg√∫n la posici√≥n que ocupa el percentil.

"""### 6Ô∏è‚É£ Agregaci√≥n"""

# TODO: Resumir las ventas por categor√≠a de producto y analizar los ingresos generados.
# Sugerencia: usar .groupby() y .agg() para generar m√©tricas como suma y promedio.

# ============================================
# 8) AGREGACI√ìN: resumen por categor√≠a
# ============================================
# Requisitos:
# - 'ventas_clean' tiene columnas 'precio', 'cantidad' y alguna columna "categor√≠a".
# ============================================

# -- Helper para detectar la columna de categor√≠a --
def encontrar_columna(df, candidatos):
    """
    df: DataFrame de pandas que queremos inspeccionar.
    candidatos: iterable de strings con patrones posibles para el nombre de la columna
                (por ejemplo: ["categoria", "cat", "segmento"]).
    Devuelve:
      - El nombre de la PRIMERA columna cuyo nombre contenga alguno de los patrones (b√∫squeda por subcadena,
        sin distinguir may√∫sculas/min√∫sculas).
      - None si no encuentra coincidencias.
    """

    # Recorremos todos los nombres de columnas del DataFrame (df.columns es un Index con esos nombres).
    for c in df.columns:

        # any(...) devuelve True si AL MENOS UNO de los elementos del generador interno es True.
        # Generador: (p in c.lower() for p in candidatos)
        #   - c.lower(): pasamos el nombre de la columna a min√∫sculas para comparar sin importar may√∫sculas/min√∫sculas.
        #   - p in c.lower(): chequea si el patr√≥n 'p' aparece como SUBCADENA dentro del nombre de la columna.
        #   - Se eval√∫a para cada 'p' en 'candidatos'.
        if any(p in c.lower() for p in candidatos):
           # Si encontramos una coincidencia, devolvemos inmediatamente el nombre ORIGINAL de la columna.
            return c
    # Si terminamos el bucle sin encontrar coincidencias, devolvemos None para indicar "no encontrada".
    return None

# Nota did√°ctica:
# - Esto hace coincidencia por SUBCADENA (ej.: "prod" matchea "producto_sku").
# - Si quer√©s coincidencia EXACTA, us√° igualdad (c.lower() == p) o expresiones regulares.
# - Si tus datos pueden tener acentos/espacios raros, pod√©s normalizar:
#     import unicodedata, re
#     def norm(s): return re.sub(r'\s+', ' ', ''.join(ch for ch in unicodedata.normalize('NFKD', s)
#                                    if not unicodedata.combining(ch))).strip().lower()


# 1) Detectar columna de categor√≠a (acepta variantes)
cat_col = encontrar_columna(ventas_clean, ["categoria", "categor√≠a", "categoria_producto", "rubro"])
if cat_col is None:
    raise ValueError("No se encontr√≥ columna de categor√≠a (por ej. 'categoria' o 'rubro').")

# 2) Asegurar columna 'ingreso' (si no existe, crearla)
if "ingreso" not in ventas_clean.columns:
    ventas_cat = ventas_clean.assign(ingreso = ventas_clean["precio"] * ventas_clean["cantidad"])
else:
    ventas_cat = ventas_clean.copy()

# 3) Agregaci√≥n por categor√≠a con groupby + agg
resumen_cat = (
    ventas_cat
    .groupby(
        by=cat_col,      # Puede ser string o lista de strings si quisi√©ramos agrupar por varias columnas.
        dropna=False,    # Mantener grupo NaN (si hay filas sin categor√≠a).
        as_index=False   # Dejar la categor√≠a como columna normal (y no como √≠ndice).
        # observed: si cat_col es 'category' y queremos mostrar solo categor√≠as presentes ‚Üí True.
    )
    .agg(
        ingreso_total=('ingreso', 'sum'),   # Suma total de ingresos por categor√≠a.
        unidades=('cantidad', 'sum'),       # Unidades totales vendidas en la categor√≠a.
        ventas=('ingreso', 'size'),         # Cantidad de registros/filas (ventas) en la categor√≠a.
        precio_promedio=('precio', 'mean')  # Precio promedio observado en la categor√≠a.
        # Otras funciones √∫tiles: 'median','max','min','std','var','nunique'...
    )
    .sort_values(
        by='ingreso_total', # Ordenar por ingreso total
        ascending=False,    # Descendente: mayores arriba
        na_position='last', # NaN al final
        ignore_index=True   # Reindexar desde 0
    )
)

# 4) Ticket promedio por venta = ingreso_total / ventas
resumen_cat = resumen_cat.assign(
    ticket_promedio_por_venta = resumen_cat['ingreso_total'] / resumen_cat['ventas']
    # assign: crea/reescribe columnas. Alternativa: resumen_cat['ticket_promedio_por_venta'] = ...
)

print("Columna de categor√≠a detectada:", cat_col)
print("Resumen por categor√≠a (ordenado por ingreso_total):")
display(resumen_cat.head(20))

"""### 7Ô∏è‚É£ Integraci√≥n de datos, opcional, NO OBLIGATORIO"""

# TODO: Combinar los sets de datos de ventas y marketing para obtener una visi√≥n m√°s amplia de las tendencias.
# Sugerencia: usar pd.merge() especificando la clave com√∫n entre ambos DataFrames.
# Documentar cualquier observaci√≥n relevante sobre la combinaci√≥n de datos.